{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5QWUNh/NIAvbr85xIOpOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcomag416/MLDL/blob/main/evaluate_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZwxT5xz-vRVE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import sys\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download and extract project repository\n",
        "\n",
        "url= \"https://github.com/marcomag416/MLDL/archive/refs/heads/main.zip\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    #print(response.content)\n",
        "    # Open the downloaded bytes and extract them\n",
        "    with ZipFile(BytesIO(response.content)) as zip_file:\n",
        "        zip_file.extractall('./')\n",
        "    print('Download and extraction complete!')\n",
        "\n",
        "sys.path.insert(0, './MLDL-main')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_j0uQ-gvbbS",
        "outputId": "f46d7132-83f9-45f0-b355-9e5dec667112"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download and extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download cityscapes dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "cityscape_dataset_path = \"./dataset/Cityscapes/Cityspaces\"\n",
        "\n",
        "#extract zip file\n",
        "if not os.path.exists(cityscape_dataset_path):\n",
        "  print(\"Extracting dataset...\")\n",
        "  with ZipFile(\"/content/drive/MyDrive/Colab Notebooks/dataset/Cityscapes.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"./dataset\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpPveMy9vhUi",
        "outputId": "5008221c-3ebd-4b24-fe6d-9cb7a9b94272"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n"
      ],
      "metadata": {
        "id": "v7Zg3qkTvkWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import from packages\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch import nn\n",
        "\n",
        "#other imports\n",
        "from models.bisenet.build_bisenet import BiSeNet\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "me2eaty3vmnr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "JBRljnfwwAUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cityscapes(Dataset):\n",
        "    def __init__(self, root_dir, split, transforms=None, label_type='gtFine_labelTrainIds'):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transforms = transforms\n",
        "        self.label_type = label_type\n",
        "\n",
        "        self.images_dir = f\"{root_dir}/images/{split}\"\n",
        "        self.labels_dir = f\"{root_dir}/gtFine/{split}\"\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.label_paths = []\n",
        "\n",
        "        # Manually iterate over directories\n",
        "        cities = [city for city in os.listdir(self.images_dir) if os.path.isdir(f\"{self.images_dir}/{city}\")]\n",
        "        for city in cities:\n",
        "            img_dir_city = f\"{self.images_dir}/{city}\"\n",
        "            lbl_dir_city = f\"{self.labels_dir}/{city}\"\n",
        "\n",
        "            if not os.path.isdir(img_dir_city) or not os.path.isdir(lbl_dir_city):\n",
        "                continue\n",
        "\n",
        "            for img_file in os.listdir(img_dir_city):\n",
        "                if img_file.endswith('_leftImg8bit.png'):\n",
        "                    img_path = f\"{img_dir_city}/{img_file}\"\n",
        "                    lbl_file = img_file.replace('_leftImg8bit.png', f'_{self.label_type}.png')\n",
        "                    lbl_path = f\"{lbl_dir_city}/{lbl_file}\"\n",
        "\n",
        "                    if os.path.isfile(img_path) and os.path.isfile(lbl_path):\n",
        "                        self.image_paths.append(img_path)\n",
        "                        self.label_paths.append(lbl_path)\n",
        "                    else:\n",
        "                        print(f\"Warning: Image or label file not found for {img_file}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        lbl_path = self.label_paths[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = Image.open(lbl_path)\n",
        "\n",
        "        image = np.array(image)\n",
        "        label = np.array(label)\n",
        "\n",
        "        if self.transforms:\n",
        "            augmented = self.transforms(image=image, mask=label)\n",
        "            image, label = augmented['image'], augmented['mask']\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Example usage\n",
        "image_transforms = A.Compose([\n",
        "    A.Resize(512, 1024),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "city_dataset_val = Cityscapes(root_dir=cityscape_dataset_path, split='val', transforms=image_transforms)\n",
        "\n",
        "print(f\"Cityscapes_Dataset_val size: {len(city_dataset_val)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ3NxO2_vu2m",
        "outputId": "7b9a2547-9a57-48dc-b8b9-fbd262d4517d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cityscapes_Dataset_val size: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test function"
      ],
      "metadata": {
        "id": "a-ng_mL8wJoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(pred, target, num_classes):\n",
        "    ious = []\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = (pred == cls)\n",
        "        target_inds = (target == cls)\n",
        "        intersection = (pred_inds[target_inds]).sum().item()\n",
        "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))  # If there is no union, set IoU to NaN\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "\n",
        "    return np.array(ious)\n",
        "\n",
        "def eval(model, dataloader, loss_fn, device, num_classes=19):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    total = 0\n",
        "    all_ious = []  # List to store IoUs for each batch\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during inference\n",
        "        for inputs_test, targets_test in dataloader:\n",
        "            inputs_test, targets_test = inputs_test.to(device), targets_test.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs_test = model(inputs_test)  # Forward pass\n",
        "            loss = loss_fn(outputs_test, targets_test)  # Calculate the loss\n",
        "\n",
        "            test_loss += loss.item() * inputs_test.size(0)  # Accumulate the total loss\n",
        "            _, predicted_test = outputs_test.max(1)\n",
        "            total += targets_test.size(0)\n",
        "\n",
        "            # Compute IoU for this batch\n",
        "            batch_ious = compute_iou(predicted_test, targets_test, num_classes)\n",
        "            all_ious.append(batch_ious)\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = test_loss / total\n",
        "\n",
        "    # Calculate mean IoU\n",
        "    all_ious = np.array(all_ious)\n",
        "    mean_iou = np.nanmean(all_ious, axis=0)  # Mean IoU for each class\n",
        "    miou = np.nanmean(mean_iou)  # Mean IoU across all classes\n",
        "\n",
        "    return avg_loss, miou, mean_iou"
      ],
      "metadata": {
        "id": "aVmnIXXHv-TS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create model and load weights"
      ],
      "metadata": {
        "id": "BV6pV83jwMWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_weights_paths = [\n",
        "    \"./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/gaussian_jitter_epoch19.pth\",\n",
        "    \"./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/gaussian_jitter_epoch24.pth\",\n",
        "    \"./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/no_transform_epoch19.pth\",\n",
        "    \"./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/guassian_horizFlip_epoch9.pth\",\n",
        "    \"./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/guassian_horizFlip_epoch34.pth\"\n",
        "]"
      ],
      "metadata": {
        "id": "uwozfm9vwjPT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
        "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
        "\n",
        "context_path = 'resnet18'\n",
        "\n",
        "city_dataloader_val = DataLoader(city_dataset_val, batch_size=8, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize the model\n",
        "model = BiSeNet(num_classes=19, context_path=context_path).to(device)\n",
        "seg_loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
        "\n"
      ],
      "metadata": {
        "id": "Edwapp-nwUUO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pretrained_weights_path in pretrained_weights_paths:\n",
        "  model.load_state_dict(torch.load(pretrained_weights_path, map_location=torch.device(device)))\n",
        "\n",
        "  avg_loss, miou, IoUs = eval(model, city_dataloader_val, seg_loss_fn, device=device)\n",
        "  print(f\"Weight path: {pretrained_weights_path}, Loss: {avg_loss}, mIoU: {miou*100:.2f}%\")\n",
        "  print(f\"\\t IoU per class: {IoUs}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXzM6L94wqYU",
        "outputId": "d5a822d5-8cc2-4535-a489-bf34c99ef136"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight path: ./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/gaussian_jitter_epoch19.pth, Loss: 0.9503804962635041, mIoU: 22.68%\n",
            "\t IoU per class: [6.50844695e-01 5.25421103e-02 6.38499885e-01 8.69905824e-02\n",
            " 7.44240538e-02 1.72042929e-01 1.21058275e-01 3.23817538e-02\n",
            " 7.52113901e-01 1.71180236e-01 6.25625594e-01 2.10068657e-01\n",
            " 1.61449969e-03 6.24114486e-01 7.05551627e-02 0.00000000e+00\n",
            " 7.50399776e-04 1.84490662e-02 5.07862528e-03]\n",
            "\n",
            "Weight path: ./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/gaussian_jitter_epoch24.pth, Loss: 1.0274983770847321, mIoU: 20.51%\n",
            "\t IoU per class: [6.19721464e-01 1.31541137e-01 5.55791612e-01 8.75234372e-02\n",
            " 9.11490880e-02 9.28500643e-02 6.18487910e-02 3.58305333e-02\n",
            " 7.47721051e-01 1.34903874e-01 6.60428421e-01 1.18081739e-01\n",
            " 1.69192850e-04 4.87359672e-01 4.61747523e-02 2.38817534e-02\n",
            " 0.00000000e+00 2.36261906e-03 0.00000000e+00]\n",
            "\n",
            "Weight path: ./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/no_transform_epoch19.pth, Loss: 1.620270528793335, mIoU: 16.79%\n",
            "\t IoU per class: [0.39957009 0.01887631 0.50116099 0.03611604 0.03339837 0.16043461\n",
            " 0.07470546 0.02993152 0.69850658 0.09783123 0.55083534 0.2134484\n",
            " 0.02214079 0.3068725  0.02262055 0.01343867 0.00842399 0.00171178\n",
            " 0.        ]\n",
            "\n",
            "Weight path: ./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/guassian_horizFlip_epoch9.pth, Loss: 1.502610987663269, mIoU: 17.16%\n",
            "\t IoU per class: [0.36942396 0.01954173 0.57292296 0.04426663 0.06200838 0.13389814\n",
            " 0.0645856  0.02252555 0.68405927 0.0242576  0.56707328 0.2353493\n",
            " 0.00808947 0.39006953 0.04078536 0.00557926 0.         0.01585358\n",
            " 0.        ]\n",
            "\n",
            "Weight path: ./drive/MyDrive/Colab Notebooks/model_weights/bisenet_gta/guassian_horizFlip_epoch34.pth, Loss: 2.009968554496765, mIoU: 18.66%\n",
            "\t IoU per class: [3.20437781e-01 1.30579032e-02 6.10836239e-01 5.47078795e-02\n",
            " 8.33124324e-02 1.48926171e-01 1.27713159e-01 4.99294206e-02\n",
            " 6.87039945e-01 3.00752629e-02 6.29346405e-01 2.85360825e-01\n",
            " 1.43860235e-02 4.34866082e-01 2.50241988e-02 8.22174990e-04\n",
            " 0.00000000e+00 2.99377578e-02 2.18645862e-05]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}