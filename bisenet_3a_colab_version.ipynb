{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcomag416/MLDL/blob/main/bisenet_3a_colab_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X2W7O-ywfU69",
      "metadata": {
        "id": "X2W7O-ywfU69"
      },
      "source": [
        "# Preliminary code\n",
        "Feel free to delete/skip this part if run in a persistent environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7-Ci8W6afSF1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "7-Ci8W6afSF1",
        "outputId": "a07fceaa-0fa5-41a3-c3bd-99f262e1469e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.1-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.5.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.1\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#install and import wandb for data collecting\n",
        "!pip install wandb\n",
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0Pd-u3n_1gOy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pd-u3n_1gOy",
        "outputId": "830d4e7b-3d2c-4282-e6a9-c00f38f3936b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download and extraction complete!\n"
          ]
        }
      ],
      "source": [
        "#download bisenet model from official repository\n",
        "import sys\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from io import BytesIO\n",
        "model_url = \"https://github.com/ooooverflow/BiSeNet/archive/refs/heads/master.zip\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(model_url)\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    #print(response.content)\n",
        "    # Open the downloaded bytes and extract them\n",
        "    with ZipFile(BytesIO(response.content)) as zip_file:\n",
        "        zip_file.extractall('./')\n",
        "    print('Download and extraction complete!')\n",
        "\n",
        "sys.path.insert(0, './BiSeNet-master')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "irlZ-v3zqFw6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irlZ-v3zqFw6",
        "outputId": "c920a3ed-17d8-4983-d0f9-cf83ffb27efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download and extraction complete!\n"
          ]
        }
      ],
      "source": [
        "#download pytorchs1_gta from github\n",
        "\n",
        "url= \"https://github.com/marcomag416/MLDL/archive/refs/heads/main.zip\"\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    #print(response.content)\n",
        "    # Open the downloaded bytes and extract them\n",
        "    with ZipFile(BytesIO(response.content)) as zip_file:\n",
        "        zip_file.extractall('./')\n",
        "    print('Download and extraction complete!')\n",
        "\n",
        "sys.path.insert(0, './MLDL-main')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hKBrbjoN4sOx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKBrbjoN4sOx",
        "outputId": "2bfb4479-02d4-43d4-f2d5-d062b678fb25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#download cityscapes dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with ZipFile(\"/content/drive/MyDrive/Colab Notebooks/dataset/Cityscapes.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"./dataset\")\n",
        "\n",
        "cityscape_dataset_path = \"./dataset/Cityscapes/Cityspaces\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fW4Y5QFOnAW5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW4Y5QFOnAW5",
        "outputId": "e82bb199-55e1-4a5f-934b-e67257db41e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic segmentation dataset created and saved as 'gta5_segmentation_dataset.csv'\n"
          ]
        }
      ],
      "source": [
        "#download and index gta5 dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gta_path = \"/content/drive/MyDrive/Colab Notebooks/dataset/GTA5.zip\"\n",
        "\n",
        "gta_dataset_path = \"./dataset/\"\n",
        "\n",
        "#extract zip file\n",
        "with ZipFile(gta_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(gta_dataset_path)\n",
        "\n",
        "# Define paths\n",
        "images_path = gta_dataset_path + 'GTA5/images'\n",
        "labels_path = gta_dataset_path + 'GTA5/labels'\n",
        "\n",
        "# Initialize lists to hold data\n",
        "data = []\n",
        "\n",
        "# Load images and corresponding masks\n",
        "for image_filename in os.listdir(images_path):\n",
        "    if image_filename.endswith('.png'):\n",
        "        image_path = os.path.join(images_path, image_filename)\n",
        "        mask_path = os.path.join(labels_path, image_filename)\n",
        "\n",
        "        # Check if corresponding mask file exists\n",
        "        if os.path.exists(mask_path):\n",
        "            # Open image and mask to ensure they can be loaded (optional, for validation)\n",
        "            try:\n",
        "                image = Image.open(image_path)\n",
        "                mask = Image.open(mask_path)\n",
        "\n",
        "                # Add data to list\n",
        "                data.append({\n",
        "                    'image_path': image_path,\n",
        "                    'mask_path': mask_path\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {image_path} or {mask_path}: {e}\")\n",
        "\n",
        "# Create a DataFrame from the data list\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('./gta5_segmentation_dataset.csv', index=False)\n",
        "\n",
        "print(\"Semantic segmentation dataset created and saved as 'gta5_segmentation_dataset.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4n2hNEvEfioO",
      "metadata": {
        "id": "4n2hNEvEfioO"
      },
      "source": [
        "# 2b BiseNet training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M_wOtOxgoOcZ",
      "metadata": {
        "id": "M_wOtOxgoOcZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from pytorchdl_gta5.labels import GTA5Labels_TaskCV2017\n",
        "\n",
        "if __name__ != '__main__':\n",
        "    raise Exception(\"This script should not be imported; it should be run directly.\")\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Define the custom dataset class\n",
        "class GTASegmentationDataset(Dataset):\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        self.data_frame = pd.read_csv(csv_file)\n",
        "        self.transform = transform\n",
        "        self.label_mapping = self._create_label_mapping()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = self.data_frame.iloc[idx, 0]\n",
        "        mask_name = self.data_frame.iloc[idx, 1]\n",
        "\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        mask = Image.open(mask_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=np.array(image), mask=np.array(mask))\n",
        "            image, mask = augmented['image'], augmented['mask']\n",
        "\n",
        "        mask = self._map_mask(np.array(mask))\n",
        "\n",
        "        if self.transform:\n",
        "            # Convert mask to tensor without normalization\n",
        "            mask = torch.from_numpy(mask).permute(2, 0, 1).float()\n",
        "            mask = mask[0]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "    def _create_label_mapping(self):\n",
        "        label_mapping = {label.color: label.ID for label in GTA5Labels_TaskCV2017.list_}\n",
        "        label_mapping[(0, 0, 0)] = 255  # Ensure unmapped colors go to 'unlabeled'\n",
        "        return label_mapping\n",
        "\n",
        "    def _map_mask(self, mask):\n",
        "        new_mask = np.zeros_like(mask)\n",
        "        for color, label_id in self.label_mapping.items():\n",
        "            color_mask = np.all(mask == color, axis=-1)\n",
        "            new_mask[color_mask] = label_id  # Use label_id instead of color\n",
        "        return new_mask\n",
        "\n",
        "\n",
        "\n",
        "# Define paths\n",
        "csv_file = './gta5_segmentation_dataset.csv'\n",
        "\n",
        "# Define image transformations\n",
        "transform = A.Compose([\n",
        "    A.Resize(720, 1280),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "train_dataset = GTASegmentationDataset(csv_file=csv_file, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9ff9f41-0185-4d9e-9a18-58439fef42bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ff9f41-0185-4d9e-9a18-58439fef42bb",
        "outputId": "141b4194-fbfb-46ba-c9aa-c029e7669f4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Val_Dataset size: 500\n"
          ]
        }
      ],
      "source": [
        "class Cityscapes(Dataset):\n",
        "    def __init__(self, root_dir, split, transforms=None, label_type='gtFine_labelTrainIds'):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transforms = transforms\n",
        "        self.label_type = label_type\n",
        "\n",
        "        self.images_dir = f\"{root_dir}/images/{split}\"\n",
        "        self.labels_dir = f\"{root_dir}/gtFine/{split}\"\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.label_paths = []\n",
        "\n",
        "        # Manually iterate over directories\n",
        "        cities = [city for city in os.listdir(self.images_dir) if os.path.isdir(f\"{self.images_dir}/{city}\")]\n",
        "        for city in cities:\n",
        "            img_dir_city = f\"{self.images_dir}/{city}\"\n",
        "            lbl_dir_city = f\"{self.labels_dir}/{city}\"\n",
        "\n",
        "            if not os.path.isdir(img_dir_city) or not os.path.isdir(lbl_dir_city):\n",
        "                continue\n",
        "\n",
        "            for img_file in os.listdir(img_dir_city):\n",
        "                if img_file.endswith('_leftImg8bit.png'):\n",
        "                    img_path = f\"{img_dir_city}/{img_file}\"\n",
        "                    lbl_file = img_file.replace('_leftImg8bit.png', f'_{self.label_type}.png')\n",
        "                    lbl_path = f\"{lbl_dir_city}/{lbl_file}\"\n",
        "\n",
        "                    if os.path.isfile(img_path) and os.path.isfile(lbl_path):\n",
        "                        self.image_paths.append(img_path)\n",
        "                        self.label_paths.append(lbl_path)\n",
        "                    else:\n",
        "                        print(f\"Warning: Image or label file not found for {img_file}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        lbl_path = self.label_paths[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = Image.open(lbl_path)\n",
        "\n",
        "        image = np.array(image)\n",
        "        label = np.array(label)\n",
        "\n",
        "        if self.transforms:\n",
        "            augmented = self.transforms(image=image, mask=label)\n",
        "            image, label = augmented['image'], augmented['mask']\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Example usage\n",
        "image_transforms = A.Compose([\n",
        "    A.Resize(512, 1024),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_dataset = Cityscapes(root_dir=cityscape_dataset_path, split='val', transforms=image_transforms)\n",
        "\n",
        "\n",
        "print(f\"Val_Dataset size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0ed0f97-7f98-465d-9726-48c43989280f",
      "metadata": {
        "id": "d0ed0f97-7f98-465d-9726-48c43989280f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, optimizer_train, dataloader, loss_fn_train):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0.0\n",
        "    total = 0\n",
        "\n",
        "    for idx, (inputs_train, targets_train) in enumerate(dataloader):\n",
        "        inputs_train = inputs_train.to(device)\n",
        "        targets_train = targets_train.to(device, dtype=torch.long)  # Move data to the appropriate device\n",
        "\n",
        "        optimizer_train.zero_grad()  # Zero out gradients from the previous iteration\n",
        "        outputs_train, _, _ = model(inputs_train)  # Forward pass\n",
        "        # print( \"train\")\n",
        "        loss = loss_fn_train(outputs_train, targets_train)  # Calculate the loss\n",
        "\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer_train.step()  # Update the weights\n",
        "\n",
        "        wandb.log({\"train/Batch loss\": loss})\n",
        "\n",
        "        train_loss += loss.item() * inputs_train.size(0)  # Accumulate the total loss\n",
        "        _, predicted_train = outputs_train.max(1)\n",
        "        total += targets_train.size(0)\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    avg_loss = train_loss / total\n",
        "\n",
        "    wandb.log({\"train/Epoch loss\": avg_loss})\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def compute_iou(pred, target, num_classes):\n",
        "    ious = []\n",
        "    pred = pred.view(-1)\n",
        "    target = target.view(-1)\n",
        "\n",
        "    for cls in range(num_classes):\n",
        "        pred_inds = (pred == cls)\n",
        "        target_inds = (target == cls)\n",
        "        intersection = (pred_inds[target_inds]).sum().item()\n",
        "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))  # If there is no union, set IoU to NaN\n",
        "        else:\n",
        "            ious.append(intersection / union)\n",
        "\n",
        "    return np.array(ious)\n",
        "\n",
        "def eval(model, dataloader, loss_fn, device, num_classes=19):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    total = 0\n",
        "    all_ious = []  # List to store IoUs for each batch\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during inference\n",
        "        for inputs_test, targets_test in dataloader:\n",
        "            inputs_test, targets_test = inputs_test.to(device), targets_test.to(device, dtype=torch.long)\n",
        "\n",
        "            outputs_test = model(inputs_test)  # Forward pass\n",
        "            loss = loss_fn(outputs_test, targets_test)  # Calculate the loss\n",
        "\n",
        "            test_loss += loss.item() * inputs_test.size(0)  # Accumulate the total loss\n",
        "            _, predicted_test = outputs_test.max(1)\n",
        "            total += targets_test.size(0)\n",
        "\n",
        "            # Compute IoU for this batch\n",
        "            batch_ious = compute_iou(predicted_test, targets_test, num_classes)\n",
        "            all_ious.append(batch_ious)\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = test_loss / total\n",
        "\n",
        "    wandb.log({})\n",
        "\n",
        "    # Calculate mean IoU\n",
        "    all_ious = np.array(all_ious)\n",
        "    mean_iou = np.nanmean(all_ious, axis=0)  # Mean IoU for each class\n",
        "    miou = np.nanmean(mean_iou)  # Mean IoU across all classes\n",
        "\n",
        "    wandb.log({\"val/Validation loss\": avg_loss, \"val/mIoU\": miou})\n",
        "\n",
        "    return avg_loss, miou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d68301fd-48a9-43f7-8749-f3ef6974be40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d68301fd-48a9-43f7-8749-f3ef6974be40",
        "outputId": "89613611-e3f9-4d66-ba87-16bcb51ff8ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 182MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:01<00:00, 159MB/s]\n"
          ]
        }
      ],
      "source": [
        "from model.build_BiSeNet import BiSeNet\n",
        "from torch import nn\n",
        "from torch.optim.lr_scheduler import PolynomialLR\n",
        "\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
        "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
        "\n",
        "context_path = 'resnet18'\n",
        "\n",
        "#save hyperparameters\n",
        "config = {\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"max_epochs\": 50,\n",
        "    \"batch_size\": 8,\n",
        "    \"weight_decay\": \"None\",\n",
        "    \"dataset\": \"Cityscapes\",\n",
        "    \"scheduler\": \"None\",\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"transformations\": \"None\"\n",
        "}\n",
        "\n",
        "\n",
        "# create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "\n",
        "# Initialize the model\n",
        "model = BiSeNet(num_classes=19, context_path=context_path).to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
        "#scheduler = PolynomialLR(optimizer, total_iters=config[\"max_epochs\"], power=config[\"polyPower\"])\n",
        "scheduler = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D_Otmc0e9BFr",
      "metadata": {
        "id": "D_Otmc0e9BFr"
      },
      "outputs": [],
      "source": [
        "#run names\n",
        "project_name = \"bisenet_gta\"\n",
        "run_name = \"no_transform\"\n",
        "run_id = \"29qdj0c0\"\n",
        "\n",
        "#set epoch to 0 to start training from scratch\n",
        "load_epoch = 9\n",
        "\n",
        "weight_path = f\"./drive/MyDrive/Colab Notebooks/model_weights/{project_name}/{run_name}_epoch{load_epoch}.pth\"\n",
        "state_path = f\"./drive/MyDrive/Colab Notebooks/model_weights/{project_name}/{run_name}_epoch{load_epoch}.dict\"\n",
        "resume = \"never\"\n",
        "id = None\n",
        "if(load_epoch > 0):\n",
        "  resume=\"must\"\n",
        "  model.load_state_dict(torch.load(weight_path))\n",
        "  optimizer.load_state_dict(torch.load(state_path))\n",
        "  id = run_id\n",
        "  load_epoch += 1 #start training from next epoch\n",
        "\n",
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3387c21-5c4e-4de8-9fe4-62c8fa5cac85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "b3387c21-5c4e-4de8-9fe4-62c8fa5cac85",
        "outputId": "69c6738d-4726-4445-cd65-2e3b28187383"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarcomag416\u001b[0m (\u001b[33mmarco-magnanini\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240612_071603-29qdj0c0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Resuming run <strong><a href='https://wandb.ai/marco-magnanini/bisenet_gta/runs/29qdj0c0' target=\"_blank\">no_transform</a></strong> to <a href='https://wandb.ai/marco-magnanini/bisenet_gta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/marco-magnanini/bisenet_gta' target=\"_blank\">https://wandb.ai/marco-magnanini/bisenet_gta</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/marco-magnanini/bisenet_gta/runs/29qdj0c0' target=\"_blank\">https://wandb.ai/marco-magnanini/bisenet_gta/runs/29qdj0c0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Loss: 1.2199229459762573, mIoU: 18.16%\n",
            "Epoch: 12, Loss: 1.5381306076049805, mIoU: 18.35%\n",
            "Epoch: 13, Loss: 1.3869496812820434, mIoU: 17.37%\n",
            "Epoch: 14, Loss: 1.8458681802749635, mIoU: 15.26%\n",
            "Epoch: 15, Loss: 1.2882082891464233, mIoU: 19.27%\n",
            "Epoch: 16, Loss: 1.3030412702560425, mIoU: 15.70%\n",
            "Epoch: 17, Loss: 1.3055522546768188, mIoU: 18.00%\n",
            "Epoch: 18, Loss: 1.460644892692566, mIoU: 16.97%\n",
            "Epoch: 19, Loss: 1.4215009231567384, mIoU: 17.93%\n",
            "Epoch: 20, Loss: 1.6232257385253905, mIoU: 16.79%\n",
            "Epoch: 21, Loss: 1.6584154977798462, mIoU: 16.72%\n",
            "Epoch: 22, Loss: 1.478221254348755, mIoU: 17.49%\n",
            "Epoch: 23, Loss: 1.6094819869995116, mIoU: 15.39%\n",
            "Epoch: 24, Loss: 1.5905858163833617, mIoU: 18.41%\n",
            "Epoch: 25, Loss: 1.6459529752731323, mIoU: 17.64%\n",
            "Epoch: 26, Loss: 1.304228388786316, mIoU: 19.65%\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "#start wandb run\n",
        "wandb.init(id=id, project=project_name, name=run_name, config=config, resume=resume)\n",
        "\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "for epoch in range(load_epoch, config[\"max_epochs\"]):\n",
        "    wandb.log({\"Epoch\": epoch+1})\n",
        "    train(model, optimizer, train_dataloader, loss_fn)\n",
        "    avg_loss, miou = eval(model, val_dataloader, loss_fn, device=device)\n",
        "    if scheduler != None:\n",
        "      scheduler.step()\n",
        "    #save model state every 5 epochs\n",
        "    if((epoch + 1) % 5 == 0):\n",
        "      torch.save(model.state_dict(), f\"./drive/MyDrive/Colab Notebooks/model_weights/{project_name}/{run_name}_epoch{epoch}.pth\")\n",
        "      torch.save(optimizer.state_dict(),  f\"./drive/MyDrive/Colab Notebooks/model_weights/{project_name}/{run_name}_epoch{epoch}.dict\")\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {avg_loss}, mIoU: {miou*100:.2f}%\")\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time - start_time:.3f} seconds\")\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}