{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6be309-74df-4b2a-ba3d-fc1d26aef3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic segmentation dataset created and saved as 'gta5_segmentation_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths\n",
    "images_path = 'GTA5/GTA5/images'\n",
    "labels_path = 'GTA5/GTA5/labels'\n",
    "\n",
    "# Initialize lists to hold data\n",
    "data = []\n",
    "\n",
    "# Load images and corresponding masks\n",
    "for image_filename in os.listdir(images_path):\n",
    "    if image_filename.endswith('.png'):\n",
    "        image_path = os.path.join(images_path, image_filename)\n",
    "        mask_path = os.path.join(labels_path, image_filename)\n",
    "        \n",
    "        # Check if corresponding mask file exists\n",
    "        if os.path.exists(mask_path):\n",
    "            # Open image and mask to ensure they can be loaded (optional, for validation)\n",
    "            try:\n",
    "                image = Image.open(image_path)\n",
    "                mask = Image.open(mask_path)\n",
    "                \n",
    "                # Add data to list\n",
    "                data.append({\n",
    "                    'image_path': image_path,\n",
    "                    'mask_path': mask_path\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {image_path} or {mask_path}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('gta5_segmentation_dataset.csv', index=False)\n",
    "\n",
    "print(\"Semantic segmentation dataset created and saved as 'gta5_segmentation_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1126f400-c92b-49e5-a0f5-a120e006da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from pytorchdl_gta5.labels import GTA5Labels_TaskCV2017\n",
    "\n",
    "if __name__ != '__main__':\n",
    "    raise Exception(\"This script should not be imported; it should be run directly.\")\n",
    "\n",
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define the custom dataset class\n",
    "class GTASegmentationDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.label_mapping = self._create_label_mapping()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        img_name = self.data_frame.iloc[idx, 0]\n",
    "        mask_name = self.data_frame.iloc[idx, 1]\n",
    "    \n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        mask = Image.open(mask_name).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "            # Resize mask to match the size of the transformed image\n",
    "            mask = transforms.Resize(image.shape[1:])(mask)\n",
    "            \n",
    "        mask = self._map_mask(np.array(mask))\n",
    "    \n",
    "        if self.transform:\n",
    "            # Convert mask to tensor without normalization\n",
    "            mask = torch.from_numpy(mask).permute(2, 0, 1).float()\n",
    "            mask = mask[0]\n",
    "    \n",
    "        return image, mask\n",
    "\n",
    "    def _create_label_mapping(self):\n",
    "        label_mapping = {label.color: label.ID for label in GTA5Labels_TaskCV2017.list_}\n",
    "        label_mapping[(0, 0, 0)] = 255  # Ensure unmapped colors go to 'unlabeled'\n",
    "        return label_mapping\n",
    "        \n",
    "    def _map_mask(self, mask):\n",
    "        new_mask = np.zeros_like(mask)\n",
    "        for color, label_id in self.label_mapping.items():\n",
    "            color_mask = np.all(mask == color, axis=-1)\n",
    "            new_mask[color_mask] = label_id  # Use label_id instead of color\n",
    "        return new_mask\n",
    "\n",
    "    \n",
    "\n",
    "# Define paths\n",
    "csv_file = 'gta5_segmentation_dataset.csv'\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((1056, 1920)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "train_dataset = GTASegmentationDataset(csv_file=csv_file, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d574c1-75f8-4046-8653-1ed0ef6d3ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_Dataset size: 500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class Cityscapes(Dataset):\n",
    "    def __init__(self, root_dir, split, transforms=None, label_type='gtFine_labelTrainIds'):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "        self.label_type = label_type\n",
    "\n",
    "        self.images_dir = f\"{root_dir}/images/{split}\"\n",
    "        self.labels_dir = f\"{root_dir}/gtFine/{split}\"\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.label_paths = []\n",
    "\n",
    "        # Manually iterate over directories\n",
    "        cities = [city for city in os.listdir(self.images_dir) if os.path.isdir(f\"{self.images_dir}/{city}\")]\n",
    "        for city in cities:\n",
    "            img_dir_city = f\"{self.images_dir}/{city}\"\n",
    "            lbl_dir_city = f\"{self.labels_dir}/{city}\"\n",
    "\n",
    "            if not os.path.isdir(img_dir_city) or not os.path.isdir(lbl_dir_city):\n",
    "                continue\n",
    "\n",
    "            for img_file in os.listdir(img_dir_city):\n",
    "                if img_file.endswith('_leftImg8bit.png'):\n",
    "                    img_path = f\"{img_dir_city}/{img_file}\"\n",
    "                    lbl_file = img_file.replace('_leftImg8bit.png', f'_{self.label_type}.png')\n",
    "                    lbl_path = f\"{lbl_dir_city}/{lbl_file}\"\n",
    "\n",
    "                    if os.path.isfile(img_path) and os.path.isfile(lbl_path):\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.label_paths.append(lbl_path)\n",
    "                    else:\n",
    "                        print(f\"Warning: Image or label file not found for {img_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        lbl_path = self.label_paths[idx]\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = Image.open(lbl_path)\n",
    "\n",
    "        image = np.array(image)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        if self.transforms:\n",
    "            augmented = self.transforms(image=image, mask=label)\n",
    "            image, label = augmented['image'], augmented['mask']\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Example usage\n",
    "image_transforms = A.Compose([\n",
    "    A.Resize(512, 1024),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_dataset = Cityscapes(root_dir='Cityscapes/Cityspaces', split='val', transforms=image_transforms)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "print(f\"Val_Dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafae0dd-8050-489b-a4b3-b1eb96ebef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_bisenet import BiSeNet\n",
    "from torch import nn\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "context_path = 'resnet18'\n",
    "\n",
    "# Initialize the model\n",
    "model = BiSeNet(num_classes=19, context_path=context_path).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Set the manual seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4ccf9da-fad9-4c23-8e70-b74fc12afaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer_train, dataloader, loss_fn_train):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    total = 0\n",
    "\n",
    "    for idx, (inputs_train, targets_train) in enumerate(dataloader):\n",
    "        inputs_train = inputs_train.to(device)\n",
    "        targets_train = targets_train.to(device, dtype=torch.long)  # Move data to the appropriate device\n",
    "\n",
    "        optimizer_train.zero_grad()  # Zero out gradients from the previous iteration\n",
    "        outputs_train, _, _ = model(inputs_train)  # Forward pass\n",
    "        # print(outputs_train.shape, targets_train.shape)\n",
    "        loss = loss_fn_train(outputs_train, targets_train)  # Calculate the loss\n",
    "        \n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer_train.step()  # Update the weights\n",
    "\n",
    "        train_loss += loss.item() * inputs_train.size(0)  # Accumulate the total loss\n",
    "        _, predicted_train = outputs_train.max(1)\n",
    "        total += targets_train.size(0)\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = train_loss / total\n",
    "    \n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def compute_iou(pred, target, num_classes):\n",
    "    ious = []\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    for cls in range(num_classes):\n",
    "        pred_inds = (pred == cls)\n",
    "        target_inds = (target == cls)\n",
    "        intersection = (pred_inds[target_inds]).sum().item()\n",
    "        union = pred_inds.sum().item() + target_inds.sum().item() - intersection\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # If there is no union, set IoU to NaN\n",
    "        else:\n",
    "            ious.append(intersection / union)\n",
    "    \n",
    "    return np.array(ious)\n",
    "\n",
    "def eval(model, dataloader, loss_fn, device, num_classes=19):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    total = 0\n",
    "    all_ious = []  # List to store IoUs for each batch\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation during inference\n",
    "        for inputs_test, targets_test in dataloader:\n",
    "            inputs_test, targets_test = inputs_test.to(device), targets_test.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs_test = model(inputs_test)  # Forward pass\n",
    "            loss = loss_fn(outputs_test, targets_test)  # Calculate the loss\n",
    "\n",
    "            test_loss += loss.item() * inputs_test.size(0)  # Accumulate the total loss\n",
    "            _, predicted_test = outputs_test.max(1)\n",
    "            total += targets_test.size(0)\n",
    "\n",
    "            # Compute IoU for this batch\n",
    "            batch_ious = compute_iou(predicted_test, targets_test, num_classes)\n",
    "            all_ious.append(batch_ious)\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = test_loss / total\n",
    "    \n",
    "    # Calculate mean IoU\n",
    "    all_ious = np.array(all_ious)\n",
    "    mean_iou = np.nanmean(all_ious, axis=0)  # Mean IoU for each class\n",
    "    miou = np.nanmean(mean_iou)  # Mean IoU across all classes\n",
    "    \n",
    "    return avg_loss, miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d545a-25f5-4e88-8d57-0c47141cd602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Setup training and save the results\n",
    "for _ in range(50):\n",
    "    train(model, optimizer, train_dataloader, loss_fn)\n",
    "    avg_loss, miou = eval(model, val_dataloader, loss_fn, device=device)\n",
    "    print(f\"Loss: {avg_loss}, mIoU: {miou*100:.2f}%\")\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Total training time: {end_time - start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1411a9-ede3-4036-bc6a-50eb0cea390a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
